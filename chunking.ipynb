{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b779cdd9",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61daed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open (\"人工客服设置SOP.json\", 'r+') as f:\n",
    "    chunks1 = []\n",
    "    for i in json.load(f):\n",
    "        chunks1.append({'text':i[\"text\"].strip(\"。\") + '。'})\n",
    "with open (\"工单管理.json\", 'r+') as f:\n",
    "    chunks2 = []\n",
    "    for i in json.load(f):\n",
    "        chunks2.append({'text':i[\"text\"].strip(\"。\") + '。'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0914d9c2",
   "metadata": {},
   "source": [
    "# Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7190b7c",
   "metadata": {},
   "source": [
    "install https://github.com/rango-ramesh/advanced-chunker\n",
    "\n",
    "\n",
    "replace core.py in SemanticChunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a4db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_chunker.core import SemanticChunker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "chunker1 = SemanticChunker(model_name=\"sentence-transformers/bge-m3\", max_tokens=8192, cluster_threshold=0.5, method=False)\n",
    "# all-MiniLM-L6-v2 | m3e-large | m3e-base | bge-m3\n",
    "merged_chunks = chunker1.chunk(chunks1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e76b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, merged in enumerate(merged_chunks):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(merged[\"text\"])\n",
    "    print()\n",
    "plt.plot(chunker1.get_similarity())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5844061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_chunker.core import SemanticChunker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "chunker2 = SemanticChunker(model_name=\"sentence-transformers/bge-m3\", max_tokens=8192, cluster_threshold=0.8, method=True)\n",
    "# all-MiniLM-L6-v2 | m3e-large | m3e-base | bge-m3\n",
    "merged_chunks = chunker2.chunk(chunks2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ecd58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, merged in enumerate(merged_chunks):\n",
    "    print(f\"Chunk {i}:\")\n",
    "    print(merged[\"text\"])\n",
    "    print()\n",
    "plt.plot(chunker2.get_similarity())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964646ec",
   "metadata": {},
   "source": [
    "# BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3430f389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chunk_timestamps:\n",
    "    def __init__(self,\n",
    "                 line = 0,\n",
    "                 chunk=None,\n",
    "                 start_time=None,\n",
    "                 end_time=None):\n",
    "        self.line = line\n",
    "        self.chunk = chunk\n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time\n",
    "\n",
    "    def getstamps(self):\n",
    "        return {\n",
    "                \"end_time\": self.end_time,\n",
    "                \"line\": self.line,\n",
    "                \"text\": self.chunk,\n",
    "                \"start_time\": self.start_time\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bf161f",
   "metadata": {},
   "source": [
    "### Sentence Match"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc0cc9",
   "metadata": {},
   "source": [
    "install modelscope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc84b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open (\"人工客服设置SOP.json\", 'r+') as f:\n",
    "    chunks1 = []\n",
    "    for i in json.load(f):\n",
    "        chunks1.append({'text':i[\"text\"].strip(\"。\") + '。'})\n",
    "with open (\"工单管理.json\", 'r+') as f:\n",
    "    chunks2 = []\n",
    "    for i in json.load(f):\n",
    "        chunks2.append({'text':i[\"text\"].strip(\"。\") + '。'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f8158e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope.utils.constant import Tasks\n",
    "from modelscope.outputs import OutputKeys\n",
    "from modelscope.pipelines import pipeline\n",
    "\n",
    "\n",
    "documents1 = \"\"\n",
    "for i in chunks1:\n",
    "    documents1 += i[\"text\"]\n",
    "\n",
    "p = pipeline(\n",
    "    task=Tasks.document_segmentation,\n",
    "    model='modelscope/Seqmodel', model_revision='master')\n",
    "\n",
    "result = p(documents1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755df41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [chunk.strip(\"\\t\") for chunk in result[OutputKeys.TEXT].split(\"\\n\") if len(chunk.strip(\"\\t\")) > 0]\n",
    "import json\n",
    "\n",
    "with open(\"工单管理.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "chunk_timestamps = []\n",
    "line = 0\n",
    "for chunk in chunks:\n",
    "    newchunk = Chunk_timestamps(chunk=chunk, line=line)\n",
    "    found1 = False\n",
    "    found2 = False\n",
    "    for entry in data:\n",
    "        if chunk.split(\"。\")[0] in entry[\"text\"]:\n",
    "            newchunk.start_time = entry[\"start_time\"]\n",
    "            found1 = True\n",
    "        if chunk.split(\"。\")[-2] in entry[\"text\"]:\n",
    "            newchunk.end_time = entry[\"end_time\"]\n",
    "            found2 = True\n",
    "        if found1 and found2:\n",
    "            found1 = False\n",
    "            found1 = False\n",
    "            break\n",
    "    if not found1 and not found2:\n",
    "        chunk_timestamps.append({\n",
    "            \"end_time\": None,\n",
    "            \"text\": chunk,\n",
    "            \"start_time\": None\n",
    "        })\n",
    "    chunk_timestamps.append(newchunk)\n",
    "    line += 1\n",
    "\n",
    "for item in chunk_timestamps:\n",
    "    print(item.getstamps())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2f646",
   "metadata": {},
   "source": [
    "### Word Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b969dedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open (\"response2.json\", 'r+') as f:\n",
    "    chunks1 = []\n",
    "    for i in json.load(f):\n",
    "        chunks1.append({'text':i[\"text\"].strip(\"。\") + '。'})\n",
    "\n",
    "with open(\"response2.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd4915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelscope.utils.constant import Tasks\n",
    "from modelscope.outputs import OutputKeys\n",
    "from modelscope.pipelines import pipeline\n",
    "\n",
    "\n",
    "documents1 = \"\"\n",
    "for i in chunks1:\n",
    "    documents1 += i[\"text\"]\n",
    "\n",
    "p = pipeline(\n",
    "    task=Tasks.document_segmentation,\n",
    "    model='modelscope/Seqmodel', model_revision='master')\n",
    "\n",
    "result = p(documents1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8fc88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = [chunk.strip(\"\\t\") for chunk in result[OutputKeys.TEXT].split(\"\\n\") if len(chunk.strip(\"\\t\")) > 0]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b21a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seconds_to_srt_time(seconds):\n",
    "    hours = int(seconds // 3600)\n",
    "    minutes = int((seconds % 3600) // 60)\n",
    "    secs = int(seconds % 60)\n",
    "    millis = int(round((seconds - int(seconds)) * 1000))\n",
    "    return f\"{hours:02}:{minutes:02}:{secs:02}.{millis:03}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d348fd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_timestamps = []\n",
    "line = 0\n",
    "for chunk in chunks:\n",
    "    newchunk = Chunk_timestamps(chunk=chunk, line=line)\n",
    "    found1 = False\n",
    "    found2 = False\n",
    "    for entry in data:\n",
    "        if chunk.split(\"。\")[0] in entry[\"text\"]:\n",
    "            for word in entry[\"words\"]:\n",
    "                if chunk.split(\"。\")[0][0] == word[\"word\"][0]:\n",
    "                    newchunk.start_time = seconds_to_srt_time(word[\"start\"])\n",
    "                    found1 = True\n",
    "        if chunk.split(\"。\")[-2] in entry[\"text\"]:\n",
    "            for word in entry[\"words\"]:\n",
    "                if chunk.split(\"。\")[-2][-1] == word[\"word\"][0]:\n",
    "                    newchunk.end_time = seconds_to_srt_time(word[\"end\"])\n",
    "                    found2 = True\n",
    "        if found1 and found2:\n",
    "            found1 = False\n",
    "            found1 = False\n",
    "            break\n",
    "    if not found1 and not found2:\n",
    "        chunk_timestamps.append({\n",
    "            \"end_time\": None,\n",
    "            \"text\": chunk,\n",
    "            \"start_time\": None\n",
    "        })\n",
    "    chunk_timestamps.append(newchunk)\n",
    "    line += 1\n",
    "\n",
    "for item in chunk_timestamps:\n",
    "    print(item.getstamps())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5608bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"results/result1.json\", \"w+\", encoding=\"utf-8\") as f:\n",
    "    json.dump([item.getstamps() for item in chunk_timestamps], f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae4cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"results/result1.json\", \"r+\", encoding=\"utf-8\") as f:\n",
    "    infos = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb81f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "cnt = 0\n",
    "video = \"test/62beb8064262ea272ccb6e94cfc7fb6e.mp4\"\n",
    "for info in infos:\n",
    "    start_time = info[\"start_time\"]\n",
    "    end_time = info[\"end_time\"]\n",
    "    output = \"test/result\" + str(cnt) + \".mp4\"\n",
    "    ffmpeg_command1 = \"D:/desktop/1/work/ai4c/stt/ffmpeg -y -i \" +  video\n",
    "    ffmpeg_command2= \" -ss \" + start_time\n",
    "    ffmpeg_command3 = \" -to \" + end_time + \" -c copy\"\n",
    "    ffmpeg_command4 = \" -vcodec libx264 -acodec libmp3lame \" + output\n",
    "    ffmpeg_command = ffmpeg_command1 + ffmpeg_command2 + ffmpeg_command3 + ffmpeg_command4\n",
    "    print(ffmpeg_command)\n",
    "    p = subprocess.Popen(ffmpeg_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    out, err = p.communicate()\n",
    "    print(out.decode(), err.decode())\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946a9378",
   "metadata": {},
   "source": [
    "# Late Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a8e42a",
   "metadata": {},
   "source": [
    "install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dcc003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open (\"人工客服设置SOP.json\", 'r+') as f:\n",
    "    chunks1 = []\n",
    "    for i in json.load(f):\n",
    "        chunks1.append({'text':i[\"text\"].strip(\"。\") + '。'})\n",
    "with open (\"工单管理.json\", 'r+') as f:\n",
    "    chunks2 = []\n",
    "    for i in json.load(f):\n",
    "        chunks2.append({'text':i[\"text\"].strip(\"。\") + '。'})\n",
    "\n",
    "documents1 = \"\"\n",
    "for i in chunks1:\n",
    "    documents1 += i[\"text\"]\n",
    "\n",
    "documents2 = \"\"\n",
    "for i in chunks2:\n",
    "    documents2 += i[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65df735",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/jina-embeddings-v2-base-zh', trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained('sentence-transformers/jina-embeddings-v2-base-zh', trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5192effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_by_sentences(input_text: str, tokenizer,  split_token = \"。\"):\n",
    "    \"\"\"文本切块+找到每个文本块在token粒度的索引范围\"\"\"\n",
    "    print(\"input_text:\", input_text)\n",
    "    inputs = tokenizer(input_text, return_tensors='pt', return_offsets_mapping=True)\n",
    "    punctuation_mark_id = tokenizer.convert_tokens_to_ids(split_token)\n",
    "    token_offsets = inputs['offset_mapping'][0]\n",
    "    # 保证最后的句子保存起来\n",
    "    sep_id = int(token_offsets[-1][0])\n",
    "    token_ids = inputs['input_ids'][0]\n",
    "    # 找到文本粒度的划分\n",
    "    chunk_positions = []\n",
    "    for i, (token_id, (start, end)) in enumerate(zip(token_ids, token_offsets)):\n",
    "        if token_id == punctuation_mark_id:\n",
    "            if token_offsets[i + 1][0] - token_offsets[i][1] >= 0 or token_offsets[i + 1][0]==sep_id:\n",
    "                chunk_positions.append((i, int(start + 1))) \n",
    "    chunks = [\n",
    "        input_text[x[1] : y[1]]\n",
    "        for x, y in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    span_annotations = [\n",
    "        (x[0], y[0]) for (x, y) in zip([(1, 0)] + chunk_positions[:-1], chunk_positions)\n",
    "    ]\n",
    "    return chunks, span_annotations\n",
    "\n",
    "\n",
    "split_token = \"。\"\n",
    "chunks, span_annotations = chunk_by_sentences(documents2, tokenizer, split_token=split_token) \n",
    "print('Chunks:\\n- \"' + '\"\\n- \"'.join(chunks) + '\"')\n",
    "print(span_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_pooling(\n",
    "    model_output, span_annotation: list, max_length=None\n",
    "):\n",
    "    \"\"\"对token embedding序列分chunk做mean pooling\"\"\"\n",
    "    token_embeddings = model_output[0]\n",
    "    outputs = []\n",
    "    for embeddings, annotations in zip(token_embeddings, span_annotation):\n",
    "        if (\n",
    "            max_length is not None\n",
    "        ):  # remove annotations which go bejond the max-length of the model\n",
    "            annotations = [\n",
    "                (start, min(end, max_length - 1))\n",
    "                for (start, end) in annotations\n",
    "                if start < (max_length - 1)\n",
    "            ]\n",
    "        pooled_embeddings = [\n",
    "            embeddings[start:end].sum(dim=0) / (end - start)\n",
    "            for start, end in annotations\n",
    "            if (end - start) >= 1\n",
    "        ]\n",
    "        pooled_embeddings = [\n",
    "            embedding.detach().cpu().numpy() for embedding in pooled_embeddings\n",
    "        ]\n",
    "        outputs.append(pooled_embeddings)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "inputs = tokenizer(documents2, return_tensors='pt')\n",
    "model_output = model(**inputs)\n",
    "embeddings = chunked_pooling(model_output, [span_annotations])[0]\n",
    "embeddings_traditional_chunking = model.encode(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e420f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "threshold = 0.96\n",
    "cos_sim = lambda x, y: np.dot(x, y) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "new_chunks = []\n",
    "new_chunk = [chunks[0]]\n",
    "new_similarities = []\n",
    "similarities = []\n",
    "length = len(embeddings) - 1\n",
    "i = 0\n",
    "while i < length:\n",
    "    new_similarities.append(cos_sim(embeddings[i], embeddings[i+1]))\n",
    "    similarities.append(cos_sim(embeddings_traditional_chunking[i], embeddings_traditional_chunking[i+1]))\n",
    "    if cos_sim(embeddings[i], embeddings[i+1]) > threshold:\n",
    "        new_chunk.append(chunks[i+1])\n",
    "        i += 1\n",
    "    else:\n",
    "        new_chunks.append(new_chunk)\n",
    "        new_chunk = [chunks[i+1]]\n",
    "        i += 1\n",
    "if len(new_chunk) >= 1:\n",
    "    new_chunks.append(new_chunk)\n",
    "print(len(new_chunks))\n",
    "new_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e2559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6cc1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(similarities, 'b-')\n",
    "plt.show()\n",
    "plt.plot(new_similarities, 'r-')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chunking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
